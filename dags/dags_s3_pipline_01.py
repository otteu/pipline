from airflow.sdk import DAG
import pendulum
import datetime

# AWS Hook ë° Python Operator ì„í¬íŠ¸
from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook
from airflow.providers.standard.operators.python import PythonOperator

# ==========================================================
# 1. ğŸ’¡ PythonOperatorê°€ í˜¸ì¶œí•  í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
# ==========================================================
# í•¨ìˆ˜ ì •ì˜ ì‹œ Connection ID ê¸°ë³¸ê°’ì„ 'dags_s3_ai'ë¡œ ì§€ì •í•©ë‹ˆë‹¤.
def s3_download_callable(aws_conn_id: str = 'dags_s3_ai'):
    """S3ì—ì„œ CSV íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜. ì¸ì¦ì€ Airflow Connectionì„ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    import pandas as pd

    s3_path = "s3://human09-2474/HDD/model_2017_ST4000DM000.csv"

    # Airflow Connectionì—ì„œ ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    # ğŸ’¡ aws_conn_idì— 'dags_s3_ai' ì‚¬ìš©
    hook = AwsBaseHook(aws_conn_id, client_type='s3')
    creds = hook.get_credentials()

    df = pd.read_csv(
        s3_path,
        storage_options={
            # ğŸ’¡ Connectionì—ì„œ ê°€ì ¸ì˜¨ í‚¤ ì‚¬ìš©
            "key": creds.access_key,
            "secret": creds.secret_key,
            "client_kwargs": {"region_name": "ap-northeast-2"}
        }
    )

    print(f"S3 íŒŒì¼ ë¡œë“œ ì™„ë£Œ. ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {len(df)}")
    print(f"S3 íŒŒì¼ ë¡œë“œ ì™„ë£Œ. Head: {df.head()}")

    return True

# ==========================================================
# 2. ğŸ“ DAG ì •ì˜ (Airflow 3.x ì˜ˆìƒ)
# ==========================================================
with DAG(
    # DAG IDëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    dag_id="dags_s3_pipline_01",
    start_date=pendulum.datetime(2025, 11, 27, tz="Asia/Seoul"),
    catchup=False
) as dag:

    s3_download = PythonOperator(
        task_id='s3_download',
        python_callable=s3_download_callable,
        # ğŸ’¡ op_kwargsë¥¼ í†µí•´ Connection ID ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
        op_kwargs={'aws_conn_id': 'dags_s3_ai'}
    )
